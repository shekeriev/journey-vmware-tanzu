#
# In this part, we will create a simple Kubernetes cluster
# 

# 
# Infrastructure Preparation
# 

# First, create a new virtual machine
# It will be used as a template for the nodes of the cluster

# Let's create one from a template to save some time
# Details about the template:
# - CentOS / 1 vCPU / 1 GB RAM / 48 GB HDD
# - root / Password1 ; vmuguser / Password1
# - https://zahariev.pro/go/vmug-tanzu

# Import the template

# Adjust the parameters to match the minimum requirements - 2 vCPUs / 2 GB RAM

# For easier communication, change the network adapter to bridged mode

# Power on the machine

# Log in as vmuguser on the console of the VM and check the IP address. Then, close the session

# Establish an SSH session to the VM

# Change the mode of SELinux to permisive for the current session
sudo setenforce 0

# Change the mode of SELinux to permisive in the configuration
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# Stop and disable the firewall
sudo systemctl disable --now firewalld

# Install some required packages
sudo dnf install -y bash-completion tc wget

# Add the Docker repository
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# Add the Kubernetes repository
cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

# Install the base packages
sudo dnf install -y docker-ce kubeadm kubectl 

# Start and enable Docker
sudo systemctl enable --now docker

# Start and enable Kubelet
sudo systemctl enable --now kubelet

# Pass bridged traffic to iptables chains
# This is a requirement for Container Network Interface (CNI) plug-ins to work
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

# Apply the changes
sudo sysctl --system

# Turn off the swap for the current session
sudo swapoff -a

# And then, turn it off permanently
sudo sed -i '/ swap / s/^/#/' /etc/fstab

# Turn off the machine
sudo shutdown now

# Create two or three (if you want to keep the exising machine unaffected) clones

# Select the VM > Invoke the context menu > Manage > Clone

# Next > Next > Full clone (or the linked one, its your choice) > Next > Set name > Finish > Close

# Now, that we have the cloned machines, we can power them on

# Set the IP address to 192.168.81.21X, where X is between 1 and 3 
# You can adjust the IP address to match your situation

# Set the hostname of all three to node-X.k8s where X is a number between 1 and 3
# sudo hostnamectl set-hostname node-X.k8s

# Add records for all three hosts in the /etc/hosts file
echo "192.168.81.211  node-1.k8s  node-1" | sudo tee -a /etc/hosts
echo "192.168.81.212  node-2.k8s  node-2" | sudo tee -a /etc/hosts
echo "192.168.81.213  node-3.k8s  node-3" | sudo tee -a /etc/hosts

# 
# Kubernetes Cluster Creation
#

# Log on to node-1 (our future master)
ssh vmuguser@192.168.81.211

# Initialize the cluster
sudo kubeadm init --apiserver-advertise-address=192.168.81.211 --pod-network-cidr 10.244.0.0/16

# Installation will finish relatively quickly

# (!) Copy somewhere the join command

# To start using our cluster, we must execute the following
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Let's check our cluster nodes (just one so far)
kubectl get nodes

# Note that it appears as not ready

# Check the pods as well
kubectl get pods -n kube-system

# Hm, most of the pods are operational, but there is one pair that is not (CoreDNS)

# Let's check why the node is not ready
kubectl describe node node-1

# Scroll to top and look for Ready and KubeletNotReady words

# It appears that there isn't any (POD) network plugin installed

# Here, we can find details on the topic:
# https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network 
# 
# Check here for a list of plugins
# https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model

# It appears, that by installing a pod network plugin, we will solve both issues

# Let's install a POD network plugin
# For this demo, we will use the Calico plugin
# More information here: https://docs.projectcalico.org/about/about-calico

# Install the Tigera Calico operator and custom resource definitions
kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

# Download the custom resource definition that we must alter before submitting to the cluster
wget https://docs.projectcalico.org/manifests/custom-resources.yaml

# Edit the custom-resources.yaml file and make the CIDR match the one we used earlier (10.244.0.0/16)
vi custom-resources.yaml

# Submit the modified resource to the cluster
kubectl create -f custom-resources.yaml

# We can watch the progress with:		
kubectl get pods --all-namespaces -w

# After a while both Calico and CoreDNS will be fully operational

# Press Ctrl + C to stop the monitoring

# Check again the status of the node
kubectl get nodes

# It should be operational and ready as well

# Close the session to node-1
exit

# Open a new session to node-2
ssh vmuguser@192.168.81.212

# (!) Remember the join command that we copied earlier, now its the time to use it
# It should have the following structure: 
# sudo kubeadm join [IP]:6443 --token [TOKEN] --discovery-token-ca-cert-hash sha256:[HASH]

# Join the node to the cluster (yours may be different)
sudo kubeadm join 192.168.81.211:6443 --token wzw1ao.6p9e87tex3kif3e6 \
        --discovery-token-ca-cert-hash sha256:c1b4b28e8b5ec5150eeac61ddac9d09271a6fbfb260cc4c135984dc1bd60739a

# Close the session to node-2
exit

# Open a new session to node-3
ssh vmuguser@192.168.81.213

# Join the node to the cluster (yours may be different)
sudo kubeadm join 192.168.81.211:6443 --token wzw1ao.6p9e87tex3kif3e6 \
        --discovery-token-ca-cert-hash sha256:c1b4b28e8b5ec5150eeac61ddac9d09271a6fbfb260cc4c135984dc1bd60739a

# Close the session to node-3
exit

# Open a new session to node-1
ssh vmuguser@192.168.81.211

# Check nodes
kubectl get nodes

# Show cluster information
kubectl cluster-info

# Wouldn't it be nice if we were able to control our new server from our host?
# Indeed, it would be. :)

# Close the session to node-1
exit

# Navigate to our home folder and then to the .kube folder

# Copy the configuration file (use your actual master/node-1 IP address here)
scp root@192.168.81.211:/etc/kubernetes/admin.conf .

# Backup the existing configuration if any
mv .\config .\config.bak

# Make the copied file the active configuration
mv .\admin.conf .\config

# Ask for cluster information but this time from the host
kubectl cluster-info

# 
# Dashboard Installation
# 

# Check the latest version and any installation instructions here:
# https://github.com/kubernetes/dashboard

# Deploy the Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml

# Check pods
kubectl get pods --all-namespaces

# Try to access the Dashboard
kubectl proxy

# Use this URL 
# http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

# We cannot log in as we do not have any valid way of doing it

# Stop the Dashboard proxy with Ctrl + C

# Create a file dashboard-admin-user.yml with the following content
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard

# Create one more file dashboard-admin-role.yml with the following content
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

# Apply both files
kubectl apply -f dashboard-admin-user.yml
kubectl apply -f dashboard-admin-role.yml

# Now, we can list the available secrets
kubectl -n kubernetes-dashboard get secret 

# Identify the one with name admin-user-token-xxxxx and ask for its details
kubectl -n kubernetes-dashboard describe secret admin-user-token-wtpbm 

# Copy the token field data

# Start the proxy again with
kubectl proxy

# Navigate to the same URL
# http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

# Use the token from earlier

# Explore the Dashboard

# Once done, close the browser tab and stop the proxy with Ctrl + C

#
# Deploy a two-pod application 
#

# Deploy the producer pod + service (backend part)
# code producer-pod.yml
# code producer-svc.yml
kubectl apply -f producer-pod.yml
kubectl apply -f producer-svc.yml

# Let's spin another one to act as a observer
# code observer-pod.yml
kubectl apply -f observer-pod.yml

# Connect to it 
kubectl exec -it observer-pod -- sh

# Install the curl command
apk add curl

# Now, check if the service is accessible by name (producer)
curl http://producer:5000

# Now, try the other names (service + namespace & FQDN) of the service
curl http://producer.default:5000
curl http://producer.default.svc.cluster.local:5000

# Notice the name of the pod

# Exit the observer session
exit

# Delete the pod 
kubectl delete -f producer-pod.yml

# And spin up a deployment with 3 replicas
# code producer-deployment.yml
kubectl apply -f producer-deployment.yml

# Check the pods
kubectl get pods

# Open session to the "observer"
kubectl exec -it observer-pod -- sh

# Now, check if the service is accessible by name (producer)
curl http://producer:5000

# Re-execute a few times and pay attention to the pod name

# Close the session
exit

# Deploy the consumer pod + service (frontend part)
# code consumer-pod.yml
# code consumer-svc.yml
kubectl apply -f consumer-pod.yml
kubectl apply -f consumer-svc.yml

# Check the pods and services
kubectl get pods
kubectl get services

# Open a browser tab to the IP address of one of the nodes + port 30001
# For example, navigate to http://192.168.81.211:30001

# Refresh a few times and pay attention to the IDs on top and bottom of the page

# Try with another IP address
# For example, navigate to http://192.168.81.213:30001

# Refresh a few times. It is working :)

# Delete the consumer pod
kubectl delete -f consumer-pod.yml

# Create the consumer deployment
# code consumer-deployment.yml
kubectl apply -f consumer-deployment.yml

# Open a browser tab to the IP address of one of the nodes + port 30001
# For example, navigate to http://192.168.81.211:30001